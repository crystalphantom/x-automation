# ðŸŽ“ AGL Framework: Complete Understanding

## Your Questions (Answered)

### Q1: "Don't workers just execute the agent? Shouldn't AGL only collect traces?"

**You're absolutely right to question this!** There are TWO different patterns:

---

## ðŸ“ The Two Integration Patterns

### Pattern A: OTLP Passive Collection (Production Use)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Web Application         â”‚
â”‚                                      â”‚
â”‚  User visits page                    â”‚
â”‚    â†“                                 â”‚
â”‚  POST /api/analyze-post              â”‚
â”‚    â†“                                 â”‚
â”‚  postAnalyzer.generate(post)  â†â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ This is YOUR code
â”‚    â†“                                 â”‚      running normally
â”‚  Return category & sentiment         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ Automatic OTLP traces
            â”‚ (Mastra sends these)
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGL Server                   â”‚
â”‚  (Just collects & stores traces)     â”‚
â”‚  - Doesn't control execution         â”‚
â”‚  - Doesn't run the agent             â”‚
â”‚  - Just observes                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This is what you were thinking!**
- âœ… Your app runs normally
- âœ… AGL passively collects execution traces
- âœ… No "worker" needed
- âœ… No duplicate execution

---

### Pattern B: Worker Queue (Training/Testing Use)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Your Web Application           â”‚
â”‚   (Runs separately, no change)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        Meanwhile, for TRAINING:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Test Harness                 â”‚
â”‚  submitTestPosts() â†’                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGL Server                   â”‚
â”‚  Queue: [task1, task2, task3...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGL Worker                   â”‚
â”‚  1. Dequeue task                     â”‚
â”‚  2. Execute agent (same code!)       â”‚
â”‚  3. Report results                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Post Analyzer Agent             â”‚
â”‚   (Same agent, different context)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This is what I built!**
- âœ… Separate from production
- âœ… Controlled testing environment
- âœ… Can batch 1000s of test cases
- âœ… Good for systematic evaluation

---

## ðŸ¤” Why Did I Build Pattern B?

**Short answer:** For **systematic training data collection**.

**Long answer:**

1. **Production OTLP** (Pattern A) gives you real user data, but:
   - âŒ Can't control what posts users send
   - âŒ Can't guarantee diverse test coverage
   - âŒ Hard to get labeled ground truth
   - âŒ Real users don't want to wait for experiments

2. **Worker Queue** (Pattern B) lets you:
   - âœ… Submit specific test cases
   - âœ… Know the expected output (ground truth)
   - âœ… Run 1000s of variations quickly
   - âœ… Compare different prompts systematically

**Analogy:**
- Pattern A = Collecting data from real customers
- Pattern B = Running controlled A/B tests in a lab

**Both are useful!** You can use BOTH:
- Use Pattern A for production traces
- Use Pattern B for systematic evaluation

---

## Q2: "Without evals, how does AGL know if output is correct?"

**You're 100% correct!** AGL **cannot** optimize without a reward signal.

### The Missing Piece: Reward / Evaluation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  What AGL Collects (Automatic)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  "Just launched our SDK!"   â”‚
â”‚ Output: category = "technology"    â”‚
â”‚ Time:   3.7 seconds                â”‚
â”‚ Tokens: 150                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
       â“ QUESTION â“
            â†“
  "Is 'technology' correct?"
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  What You MUST Add                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reward / Evaluation Signal:        â”‚
â”‚  - Ground truth comparison         â”‚
â”‚  - Human rating                    â”‚
â”‚  - Success metric                  â”‚
â”‚  - LLM-as-judge                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Without Rewards â†’ No Learning

**What AGL can do:**
- âœ… Collect execution traces
- âœ… Store input/output pairs
- âœ… Track timing and resource usage
- âœ… Organize rollouts and attempts

**What AGL CANNOT do without rewards:**
- âŒ Know if output is "good" or "bad"
- âŒ Optimize prompts
- âŒ Train with RL
- âŒ Improve agent performance

**Think of it like a student:**
- Collecting traces = Taking notes in class
- Rewards = Getting grades on homework
- Without grades, student doesn't know what to improve!

---

## ðŸŽ¯ How to Add Rewards (4 Approaches)

### Approach 1: Ground Truth Labels (What I Added)

```typescript
// test-data.ts
export const expectedClassifications = {
  "test-tech-1": { 
    category: "technology", 
    sentiment: "positive" 
  },
  // ... more labeled data
};

// evaluate-rollouts.ts
const expected = expectedClassifications[postId];
const actual = rollout.output.category;

const reward = (actual === expected.category) ? 1.0 : -1.0;

// Store reward in rollout metadata
updateRollout(rolloutId, { 
  metadata: { 
    reward: reward  // â† This is what RL uses!
  } 
});
```

**Pros:**
- âœ… Accurate
- âœ… You define what "correct" means

**Cons:**
- âŒ Requires manual labeling
- âŒ Doesn't scale beyond test set

---

### Approach 2: Human Feedback (RLHF)

```typescript
// Show human the result, get feedback
async function collectHumanFeedback(rollout) {
  const rating = await showToHuman({
    input: rollout.input.post,
    output: rollout.output.category,
  });
  
  // rating = 1 (good) to 5 (excellent)
  const reward = (rating - 3) / 2; // Scale to [-1, 1]
  
  updateRollout(rolloutId, {
    metadata: { reward: reward }
  });
}
```

**Pros:**
- âœ… Real human judgment
- âœ… Can capture nuance

**Cons:**
- âŒ Expensive
- âŒ Slow
- âŒ Doesn't scale

---

### Approach 3: Proxy Metrics (Indirect Signal)

```typescript
// Use downstream success as reward
async function calculateProxyReward(rollout) {
  const postId = rollout.input.post.id;
  
  //  Did user engage with the suggested action?
  const userEngaged = await checkUserEngagement(postId);
  
  // Did the pipeline succeed?
  const pipelineSucceeded = await checkPipelineSuccess(postId);
  
  let reward = 0;
  if (userEngaged) reward += 0.5;
  if (pipelineSucceeded) reward += 0.5;
  
  return reward;
}
```

**Pros:**
- âœ… Real-world signal
- âœ… Automatically collected

**Cons:**
- âŒ Noisy (many confounding factors)
- âŒ Delayed feedback

---

### Approach 4: LLM-as-Judge

```typescript
async function llmJudgeReward(rollout) {
  const judgePrompt = `
    Post: "${rollout.input.post.content}"
    Classified as: ${rollout.output.category}
    
    Is this classification correct?
    Categories:
    - technology: tech, code, APIs, frameworks
    - startups: fundraising, growth, hiring
    - product_management: features, users, product
    - general: everything else
    
    Answer: CORRECT or INCORRECT
  `;
  
  const judgment = await llm.generate(judgePrompt);
  return judgment.includes("CORRECT") ? 1.0 : -1.0;
}
```

**Pros:**
- âœ… Scales automatically
- âœ… No human labor

**Cons:**
- âŒ Judge LLM might be wrong
- âŒ Expensive (API calls)
- âŒ Can amplify biases

---

## ðŸ”„ Complete RL Training Loop (Corrected)

```
Step 1: COLLECT TRACES
â”œâ”€ Run agents on tasks
â”œâ”€ AGL stores execution data
â””â”€ But no learning happens yet! âŒ

Step 2: ADD REWARDS â† YOU MUST DO THIS!
â”œâ”€ Evaluate each rollout
â”œâ”€ Assign reward: good (+1) or bad (-1)
â””â”€ Store reward in rollout metadata

Step 3: TRAIN WITH RL (Python)
â”œâ”€ Algorithm reads rollouts WITH rewards
â”œâ”€ Learns: "What prompt led to +1 rewards?"
â”œâ”€ Generates better prompts
â””â”€ Stores optimized prompts as "resources"

Step 4: AGENT USES NEW PROMPTS
â”œâ”€ Fetch latest resources from AGL
â”œâ”€ Use improved prompts
â””â”€ Performance improves!

Step 5: LOOP BACK TO STEP 1
```

---

## ðŸ“Š What I Built vs What You Thought

| Aspect | What You Thought | What I Built | Reality |
|--------|------------------|--------------|---------|
| **Agent Execution** | AGL just observes | Worker executes agent | **Both valid** (2 patterns) |
| **Data Collection** | Automatic via OTLP | Worker-based queue | **Both valid** |
| **Use Case** | Production only | Training/testing | **Different purposes** |
| **Evals** | Not needed? âŒ | **REQUIRED!** âœ… | **You were right!** |
| **Rewards** | Automatic? âŒ | **Must add manually** âœ… | **Critical insight!** |

---

## âœ… Corrected Implementation Plan

### Phase 1: Data Collection (Done!)

```bash
bun run agl:submit  # Submit test posts
bun run agl:worker  # Process them
```

âœ… You have 8 rollouts with execution traces

---

### Phase 2: Evaluation (NEW! - THIS IS THE KEY)

```bash
bun run agl:eval  # Evaluate rollouts, add rewards
```

This script:
1. Compares agent output vs ground truth
2. Calculates reward (+1, 0.5, or -1)
3. Stores reward in rollout metadata

**Without this step, training cannot work!**

---

### Phase 3: Training (Future)

```python
# agl-training/train_post_analyzer.py

# 1. Get rollouts WITH REWARDS
rollouts = store.query_rollouts(limit=1000)
rollouts_with_rewards = [r for r in rollouts if r.metadata.get("evaluation.reward")]

# 2. Extract features
good_prompts = [r.prompt for r in rollouts if r.metadata["evaluation"]["reward"] > 0.5]
bad_prompts = [r.prompt for r in rollouts if r.metadata["evaluation"]["reward"] < 0]

# 3. Use RL algorithm (PPO, GRPO, etc.)
optimizer = RLOptimizer()
better_prompt = optimizer.optimize(
    good_examples=good_prompts,
    bad_examples=bad_prompts
)

# 4. Store improved prompt
store.add_resources({
    "prompts": {"post_analyzer_v2": better_prompt}
})
```

---

## ðŸŽ¯ Key Takeaways

1. **Two Patterns**: OTLP (passive) vs Worker (active) - both valid, different use cases

2. **Rewards are REQUIRED**: AGL cannot learn without evaluation/rewards

3. **You Must Add Evals**: Either:
   - Ground truth comparison (what I added)
   - Human ratings
   - Proxy metrics
   - LLM-as-judge

4. **RL is NOT automatic**: You must:
   - Collect traces âœ… (automatic)
   - Add rewards âœ… (manual - I added this)
   - Train with RL â³ (Python script - TODO)
   - Deploy improvements â³ (TODO)

---

## ðŸ“ Updated Commands

```bash
# 1. Collect data
bun run agl:submit

# 2. Evaluate (ADD REWARDS) â† NEW!
bun run agl:eval

# 3. Analyze
bun run agl:analyze

# 4. Train (after 500+ evaluated rollouts)
bun run agl:train
```

---

## â“ FAQ

**Q: Why build a worker if OTLP exists?**
A: Worker is for controlled testing. OTLP is for production data. Use both!

**Q: Can I use OTLP only?**
A: Yes! But you still need to add rewards somehow. Either:
- Collect human feedback
- Use proxy metrics (user engagement, etc.)
- Run periodic batch evaluations

**Q: Is the worker duplicating work?**
A: In a sense, yes. But it's for TESTING, not production. Production uses OTLP.

**Q: Can rewards be added retroactively?**
A: Yes! That's what `bun run agl:eval` does - it adds rewards to existing rollouts.

**Q: What if I don't have ground truth?**
A: Then you need one of the other reward approaches:
- Human rating
- LLM-as-judge
- Proxy metrics
Without rewards, you can collect data but cannot train.

---

**Your questions were excellent and helped correct my explanation. Thank you!** ðŸ™
